---
output: 
  html_document:
    theme: null
    highlight: null
    mathjax: null
---

```{r setup_main, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
library('yaml')
source('DataHelpers.R')
```

## What is this?
This is a set of parameterised r-markdown files designed to automatically detect features and build ensemble models for regression and classification prediction over time-series data.

----

At a high level, the system is being designed to run a number of sequential steps on a daily basis. Some thought has been put into how to deal with the staggered availability of time-series data, in particular at what point to run the predictive models to allow for the most up to date data and at what point and with which data to retrain them. The daily process will look like this:

Morning Process (eg. at market opening)

For each dataset:

1. Scrape latest available data (eg for stock prices, market Opening price can be used for predictions) ^1^

For each Predictor

1. Loads most recent saved models ^1^
2. Load the time series data
3. Run predictive models ^1^
4. Update Prediction Log


Evening Process (eg. after market close)

For each dataset:

1. scrape the rest of the data (eg closing price, high, low and volume are available) ^1^

For each Predictor

1. Train a new set of regression and ensemble models using the most up-to-date data for the selected date
2. Saves these models ^1^

^1^ Not implemented

----

![Summary of iterative training and prediction process](https://raw.githubusercontent.com/michaeldowd2/Datarecon/master/Images/TrainingAndPredictionSummary.JPG){ width=480px }

For each date the analysis trains a set of predictors on the most recently available data. The resulting models are then run over a segmented set of data, from which the predictions are fed into a set of ensemble models. This works to prevent overfitting whilst still allowing the underlying predictors to be trained with the most up to date data. Here's a zoomed in view of the process for just one day:

![Summary of daily training and prediction process](https://raw.githubusercontent.com/michaeldowd2/Datarecon/master/Images/DailySummary.JPG){ width=480px }

----

## Model Predictions
``` {r run model predictions, results = 'hide'}
start_time <- Sys.time()

#1 get all analyses
par_path <- parent_path()
predictors <-  list.dirs(path = paste(par_path, "./Output/", sep =""), full.names = TRUE, recursive = FALSE)
for (predictor in predictors) {
  # parameters
  paras <-  read_yaml(paste(predictor, 'parameters.yml', sep = '/'))
  rmarkdown::render(input = 'Prediction.rmd', output_dir = predictor, params = paras)
}

end_time <- Sys.time()
```

* The last prediction iterator ran at: `r start_time`
* Number of Predictions run: `r length(predictors)`
* Time taken: `r end_time - start_time` s

## Model Training
``` {r run trainings, results = 'hide'}
start_time <- Sys.time()

#1 get all analyses
par_path <- parent_path()
predictors <-  list.dirs(path = paste(par_path, "./Output/", sep =""), full.names = TRUE, recursive = FALSE)
for (predictor in predictors) {
  # parameters
  paras <-  read_yaml(paste(predictor, 'parameters.yml', sep = '/'))
  # 1 Predictions
  rmarkdown::render(input = 'Training.rmd', output_dir = predictor, params = paras)
}

end_time <- Sys.time()
```

* The last training iterator ran at: `r start_time`
* Number of predictors trained: `r length(predictors)`
* Time taken: `r end_time - start_time` s
